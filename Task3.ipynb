{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import fasttext\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__1', '__label__2'] are the labels or targets the model is predicting\n"
     ]
    }
   ],
   "source": [
    "# Modelling\n",
    "model = fasttext.train_supervised('train.ft.txt',label_prefix='__label__', thread=4, epoch = 10)\n",
    "print(model.labels, 'are the labels or targets the model is predicting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 number of records in the test set\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "test_data = open('test.ft.txt')\n",
    "test_data=test_data.readlines()\n",
    "print(len(test_data), 'number of records in the test set') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__2 Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I\\'m in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life\\'s hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\\n',\n",
       " \"__label__2 One of the best game music soundtracks - for a game I didn't really play: Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, as there's not too many of those kinds of songs in my other video game soundtracks. I must admit that one of the songs (Life-A Distant Promise) has brought tears to my eyes on many occasions.My one complaint about this soundtrack is that they use guitar fretting effects in many of the songs, which I find distracting. But even if those weren't included I would still consider the collection worth it.\\n\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review the test data\n",
    "test_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the __label__1 and __label__2 from the testset \n",
    "test_reviews = [w.replace('__label__2 ', '').replace('__label__1 ', '').replace('\\n', '') for w in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9172450000000001\n"
     ]
    }
   ],
   "source": [
    "# Use the predict function \n",
    "pred = model.predict(test_reviews)\n",
    "\n",
    "labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_data]\n",
    "pred_labels = [0 if x == ['__label__1'] else 1 for x in pred[0]]\n",
    "\n",
    "# run the accuracy measure. \n",
    "print(roc_auc_score(labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical ML algorithm \n",
    "Intro: Here below I use XGBOOST as classical ML algorithm.\n",
    "TFIDF + XGBoost, and CountVectorizer + XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split dataset\n",
    "NOTE: When running FastText with the whole dataset, the code can be ran successfully and fast, but when I use classical ML-algorithm, there were always have memory error. So, firstly I only use 5000 data in train and 500 in test to do the main work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews and labels extract\n",
    "def get_review(train,test):\n",
    "    X_train = [w.replace('__label__2 ', '').replace('__label__1 ', '').replace('\\n', '') for w in train]\n",
    "    X_test = [w.replace('__label__2 ', '').replace('__label__1 ', '').replace('\\n', '') for w in test]\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for i in range(0,len(train)):\n",
    "        temp = train[i][9]\n",
    "        y_train.append(temp)\n",
    "    for i in range(0,len(test)):\n",
    "        temp = test[i][9]\n",
    "        y_test.append(temp)\n",
    "    \n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600000 number of records in the test set\n"
     ]
    }
   ],
   "source": [
    "# Load the train data\n",
    "train_data = open('train.ft.txt')\n",
    "train_data=train_data.readlines()\n",
    "print(len(train_data), 'number of records in the test set') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small data set which can be ran succesefully in my computer\n",
    "train = train_data[:5000]\n",
    "test = test_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split reviews and labels\n",
    "Xtrain,Xtest,ytrain,ytest = get_review(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TfidfVectorizer+XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up TfidfVectorizer\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting TF-IDF to both training and test sets\n",
    "tfv.fit(Xtrain+Xtest)\n",
    "Xtrain_tfv =  tfv.transform(Xtrain) \n",
    "Xtest_tfv = tfv.transform(Xtest)\n",
    "\n",
    "Xtrain_tfv_vect = pd.concat([pd.DataFrame(Xtrain_tfv.toarray())], axis=1)\n",
    "Xtest_tfv_vect = pd.concat([pd.DataFrame(Xtest_tfv.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up XGBoost classifier\n",
    "clf_xgb = xgb.XGBClassifier(max_depth=3, colsample_bytree=0.8,\n",
    "                        subsample=0.8, nthread=10, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting xgboost on Tfidf\n",
    "clf_xgb_tfvmodel = clf_xgb.fit(Xtrain_tfv_vect, ytrain)\n",
    "y_pred_tfv = clf_xgb_tfvmodel.predict(Xtest_tfv_vect)\n",
    "score_xgb_tfv = accuracy_score(ytest, y_pred_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.804"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_xgb_tfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.80      0.80       241\n",
      "           2       0.81      0.81      0.81       259\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.80      0.80      0.80       500\n",
      "weighted avg       0.80      0.80      0.80       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest,y_pred_tfv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CountVectorizer+XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up CountVectorizer\n",
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(Xtrain+Xtest)\n",
    "Xtrain_ctv =  ctv.transform(Xtrain) \n",
    "Xvalid_ctv = ctv.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb_ctvmodel = clf_xgb.fit(Xtrain_ctv, ytrain)\n",
    "y_pred_ctv = clf_xgb_ctvmodel.predict(Xvalid_ctv)\n",
    "score_ctv_xgb = accuracy_score(ytest, y_pred_ctv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.822"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ctv_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.82      0.82       241\n",
      "           2       0.83      0.83      0.83       259\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.82      0.82      0.82       500\n",
      "weighted avg       0.82      0.82      0.82       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest,y_pred_ctv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase train data from 5000 to 500000, apply CountVectorizer+XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lager data\n",
    "train_larger = train_data[:500000]\n",
    "test_larger = test_data[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split reviews and labels\n",
    "Xtrain_l,Xtest_l,ytrain_l,ytest_l = get_review(train_larger,test_larger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(Xtrain_l+Xtest_l)\n",
    "Xtrain_ctv_l =  ctv.transform(Xtrain_l) \n",
    "Xvalid_ctv_l = ctv.transform(Xtest_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ctvmodel_l = clf_xgb.fit(Xtrain_ctv_l, ytrain_l)\n",
    "y_pred_ctv_l = clf_ctvmodel_l.predict(Xvalid_ctv_l)\n",
    "score_ctv_l = accuracy_score(ytest_l, y_pred_ctv_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8462"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ctv_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.82      0.84     24626\n",
      "           2       0.83      0.87      0.85     25374\n",
      "\n",
      "    accuracy                           0.85     50000\n",
      "   macro avg       0.85      0.85      0.85     50000\n",
      "weighted avg       0.85      0.85      0.85     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest_l,y_pred_ctv_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions:\n",
    "1. The technical reason why I have memory error: I didn't use readline(), instead, I used readlines() when I read txt file. Because readline() method will return a line from the file when called, and readlines() method will return all the lines in a file in the format of a list where each element is a line in the file, which means use readlines means more CPU and RAM taking.\n",
    "2. As the final result, I found that in this case CountVectorizer is better than TFIDF in preprocessing, and FastTest is more efficient in checking and predicting Amazon reviews.\n",
    "3. Increase the number of data can improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fully convelutional network\n",
    "Inspired by https://www.kaggle.com/kevinautin/fully-convolutional-accuracy-94-4-15-min , which can run the whole data set within 1.5 hours and no need to worry about run out of ROM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import bz2\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitReviewsLabels(lines):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for review in tqdm(lines):\n",
    "        rev = reviewToX(review)\n",
    "        label = reviewToY(review)\n",
    "        reviews.append(rev[:512])\n",
    "        labels.append(label)\n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviewToY(review):\n",
    "    return [1,0] if review.split(' ')[0] == '__label__1' else [0,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviewToX(review):\n",
    "    review = review.split(' ', 1)[1][:-1].lower()\n",
    "    review = re.sub('\\d','0',review)\n",
    "    if 'www.' in review or 'http:' in review or 'https:' in review or '.com' in review:\n",
    "        review = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3600000/3600000 [01:27<00:00, 41233.34it/s]\n",
      "100%|██████████| 400000/400000 [00:12<00:00, 31854.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load from the file\n",
    "reviews_train, y_train = splitReviewsLabels(train_data)\n",
    "reviews_test, y_test = splitReviewsLabels(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train, y_train = shuffle(reviews_train, y_train)\n",
    "reviews_test, y_test = shuffle(reviews_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up padding features\n",
    "max_features = 8192\n",
    "maxlen = 128\n",
    "embed_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(reviews_train)\n",
    "token_train = tokenizer.texts_to_sequences(reviews_train)\n",
    "token_test = tokenizer.texts_to_sequences(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(token_train, maxlen=maxlen, padding='post')\n",
    "x_test = pad_sequences(token_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "embedding_55 (Embedding)     (None, 128, 64)           524288    \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_178 (Bat (None, 128, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 128, 32)           14368     \n",
      "_________________________________________________________________\n",
      "batch_normalization_179 (Bat (None, 128, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 128, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_180 (Bat (None, 128, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 128, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_181 (Bat (None, 128, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 128, 32)           3104      \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 128, 2)            66        \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_43  (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 548,674\n",
      "Trainable params: 548,354\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(maxlen,))\n",
    "net = Embedding(max_features, embed_size)(input)\n",
    "net = Dropout(0.5)(net)\n",
    "net = BatchNormalization()(net)\n",
    "\n",
    "net = Conv1D(32, 7, padding='same', activation='relu')(net)\n",
    "net = BatchNormalization()(net)\n",
    "net = Conv1D(32, 3, padding='same', activation='relu')(net)\n",
    "net = BatchNormalization()(net)\n",
    "net = Conv1D(32, 3, padding='same', activation='relu')(net)\n",
    "net = BatchNormalization()(net)\n",
    "net = Conv1D(32, 3, padding='same', activation='relu')(net)\n",
    "net1 = BatchNormalization()(net)\n",
    "\n",
    "net = Conv1D(2, 1)(net)\n",
    "net = GlobalAveragePooling1D()(net)\n",
    "output = Activation('softmax')(net)\n",
    "model = Model(inputs = input, outputs = output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800000 samples, validate on 1800000 samples\n",
      "Epoch 1/2\n",
      "1800000/1800000 [==============================] - 1925s 1ms/step - loss: 0.1897 - acc: 0.9267 - val_loss: 0.1717 - val_acc: 0.9349\n",
      "Epoch 2/2\n",
      "1800000/1800000 [==============================] - 17920s 10ms/step - loss: 0.1745 - acc: 0.9333 - val_loss: 0.1679 - val_acc: 0.9380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1bd2f994d0>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=2048, epochs=2, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000/400000 [==============================] - 1686s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17004760593764484, 0.9375324845314026]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions:\n",
    "1. NN network is more user-frinedly in practice.\n",
    "2. When I tune some parametors such as optimizer,activation function,batch size etc. I found that there're only slight differences. Maybe I didn't try enough as re-running the code takes too much time.\n",
    "3. In different environments have different results. In Anaconda I got 93.7 acc and spent 1.4h for running, while in Kaggle, got 94 acc only need a half-hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
